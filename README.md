# Cognition

> OpenCode-style AI coding assistant. Local. Fast. Extensible.

An open-source, self-hosted coding assistant inspired by Claude Code and OpenCode, built with **LangGraph**, **FastAPI**, and **Textual TUI**.

## Features

- ğŸ§  **LangGraph-Powered** - Advanced context management with state machines
- ğŸ”Œ **Multi-LLM Support** - OpenAI, AWS Bedrock, OpenAI-compatible APIs
- ğŸ’¬ **Persistent Sessions** - Resume conversations across restarts
- ğŸ¨ **Beautiful TUI** - Terminal UI built with Textual
- âš¡ **In-Process Agent** - No Docker overhead, immediate responses
- ğŸ”’ **Privacy-First** - Code never leaves your machine
- ğŸ“Š **OTEL Ready** - Built-in observability for production deployments

## Quick Start

See [GETTING_STARTED.md](./GETTING_STARTED.md) for detailed setup instructions.

### 60-Second Setup

```bash
# Install dependencies
uv pip install -r server/requirements.txt
uv pip install -r client/requirements.txt

# Configure LLM (OpenAI example)
export OPENAI_API_KEY="sk-your-key"
export LLM_PROVIDER="openai"

# Start server (Terminal 1)
cd server && uv run uvicorn app.main:app --reload --port 8000

# Start client (Terminal 2)
cd client && uv run python -m tui.app
```

## Architecture

```
TUI Client â†â†’ WebSocket â†â†’ FastAPI Server â†â†’ In-Process Agent â†â†’ LLM
  (Textual)                 (Port 8000)        (LangGraph)      (OpenAI/Bedrock)
```

**Key Design Decisions:**
- âœ… In-process agents (no Docker complexity)
- âœ… LangGraph for context management
- âœ… Simple REST + WebSocket API
- âœ… File-based projects, SQLite-ready for sessions
- âœ… Support for client-server separation (PaaS-ready)

## Project Structure

```
cognition/
â”œâ”€â”€ server/app/              # FastAPI + agent runtime
â”‚   â”œâ”€â”€ agent/               # InProcessAgent (LangGraph)
â”‚   â”œâ”€â”€ sessions/            # Session lifecycle
â”‚   â”œâ”€â”€ projects/            # Project metadata
â”‚   â””â”€â”€ main.py              # API endpoints
â”œâ”€â”€ client/tui/              # Textual TUI
â”‚   â”œâ”€â”€ screens/             # Main screens
â”‚   â”œâ”€â”€ widgets/             # UI components
â”‚   â”œâ”€â”€ api.py               # REST client
â”‚   â””â”€â”€ websocket.py         # WebSocket handler
â”œâ”€â”€ tests/                   # 156+ unit tests
â””â”€â”€ docs/                    # Architecture & guides
```

## Development

```bash
# Run all tests
uv run pytest tests/ --ignore=tests/e2e -v

# Type checking
uv run mypy server/ client/ --strict

# Format & lint
uv run ruff format server/ client/
uv run ruff check server/ client/
```

## Configuration

See `.env.example` for all options:

```bash
# LLM Provider
LLM_PROVIDER=openai              # or "bedrock", "openai_compatible"
OPENAI_API_KEY=sk-...
DEFAULT_MODEL=gpt-4-turbo-preview

# Server
PORT=8000
LOG_LEVEL=info
```

## Status

- âœ… Phase 1: In-process agent architecture (COMPLETE)
- âœ… Phase 2: TUI client integration (COMPLETE)  
- ğŸš€ Phase 3: Documentation & polish (IN PROGRESS)

## Roadmap

- [ ] OTEL observability integration
- [ ] Code analysis tools (file read, search, etc)
- [ ] Web UI (React)
- [ ] Tool system (bash, file editing)
- [ ] Agent templates
- [ ] Multi-provider billing

## Contributing

Contributions welcome! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) first.

## License

MIT

---

**Quick Links:**
- ğŸ“– [Getting Started Guide](./GETTING_STARTED.md)
- ğŸ—ï¸ [Architecture Deep Dive](./docs/ARCHITECTURE.md)
- ğŸ§ª [Testing Guide](./docs/TESTING.md)
- ğŸ› [Troubleshooting](./GETTING_STARTED.md#troubleshooting)

# Cognition Environment Configuration

# Server Configuration
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info
DEBUG=false

# LLM Provider (openai, openai_compatible, anthropic, bedrock)
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=sk-...

# OpenAI-Compatible API Configuration (LiteLLM, vLLM, Ollama, etc.)
# Use this for any OpenAI API compatible endpoint
LLM_PROVIDER=openai_compatible
OPENAI_API_BASE=http://localhost:8000/v1
# OPENAI_API_KEY=optional-api-key  # Some local instances don't require auth
# DEFAULT_MODEL=mistral-7b-instruct  # Use the model name your proxy serves

# Anthropic Configuration
ANTHROPIC_API_KEY=sk-ant-...

# Default LLM Model (used for OpenAI/Anthropic/OpenAI-compatible)
DEFAULT_MODEL=gpt-4-turbo-preview
# DEFAULT_MODEL=claude-3-opus-20240229
# DEFAULT_MODEL=mistral-7b-instruct-v0.2  # For vLLM/Ollama

# AWS Bedrock Configuration
# Option 1: Use IAM Role (recommended for EC2/EKS/ECS)
USE_BEDROCK_IAM_ROLE=false

# Option 2: Explicit AWS Credentials
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_SESSION_TOKEN=...

# Option 3: AWS Profile
AWS_PROFILE=default

# AWS Region for Bedrock
AWS_REGION=us-east-1

# Bedrock Model ID
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# BEDROCK_MODEL_ID=anthropic.claude-3-opus-20240229-v1:0
# BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
# BEDROCK_MODEL_ID=amazon.titan-text-express-v1
# BEDROCK_MODEL_ID=meta.llama3-70b-instruct-v1:0

# Container Configuration
DOCKER_IMAGE=opencode-agent:py
CONTAINER_TIMEOUT=300
CONTAINER_MEMORY_LIMIT=2g
CONTAINER_CPU_LIMIT=1.0

# Workspace Configuration
WORKSPACE_ROOT=./workspaces
MAX_SESSIONS=100

# ============================================================================
# Project & Session Persistence
# ============================================================================

# Maximum number of projects
MAX_PROJECTS=1000

# Auto-cleanup configuration
PROJECT_CLEANUP_ENABLED=true
PROJECT_CLEANUP_AFTER_DAYS=30
PROJECT_CLEANUP_WARNING_DAYS=3
PROJECT_CLEANUP_CHECK_INTERVAL=86400  # seconds (24 hours)

# Memory persistence (hybrid hot/persistent strategy)
MEMORY_SNAPSHOT_ENABLED=true
MEMORY_SNAPSHOT_INTERVAL=300  # seconds (5 minutes)

# Container lifecycle
CONTAINER_STOP_ON_DISCONNECT=true
CONTAINER_RECREATE_ON_RECONNECT=true

# ============================================================================
# Backend Routes Configuration (CompositeBackend)
# ============================================================================
#
# Configures path-based routing for the agent's virtual filesystem
# 
# Three backend types available:
#   - filesystem: Direct disk access via Docker volume mounts (zero-copy, fast)
#   - store: In-memory persistent storage across tool calls (survives session)
#   - state: Runtime ephemeral state (fast, cleared on restart)
#
# Format: JSON string with virtual path → backend config mapping
#
# ============================================================================
# RECOMMENDED CONFIGURATION (Hybrid Memory Strategy):
# ============================================================================
# AGENT_BACKEND_ROUTES='{ "/workspace/": {"type": "filesystem"}, "/memories/hot/": {"type": "store"}, "/memories/persistent/": {"type": "filesystem"}, "/tmp/": {"type": "state"} }'
#
# This provides:
#   - /workspace/ → Filesystem (code files, zero-copy to container)
#   - /memories/hot/ → RAM (fast, ephemeral, snapshotted every 5 min)
#   - /memories/persistent/ → Disk (permanent, restored on reconnect)
#   - /tmp/ → Runtime state (fastest, completely ephemeral)
#
# ============================================================================
# Alternative Configurations:
# ============================================================================
#
# 1. Standard setup (no persistent memories):
# AGENT_BACKEND_ROUTES='{ "/workspace/": {"type": "filesystem", "virtual_mode": true}, "/memories/": {"type": "store"} }'
#
# 2. Fully persistent memories (all memories on disk):
# AGENT_BACKEND_ROUTES='{ "/workspace/": {"type": "filesystem"}, "/memories/": {"type": "filesystem"}, "/tmp/": {"type": "state"} }'
#
# 3. Multiple data locations:
# AGENT_BACKEND_ROUTES='{ "/workspace/": {"type": "filesystem"}, "/data/": {"type": "filesystem", "root": "/mnt/shared"}, "/memories/hot/": {"type": "store"}, "/memories/persistent/": {"type": "filesystem"}, "/cache/": {"type": "state"} }'
#
# ============================================================================
# Backend Type Details:
# ============================================================================
#
# FilesystemBackend (/workspace/, /data/, /memories/persistent/):
#   - Maps to actual directories on disk
#   - Direct file I/O (0.036ms write latency)
#   - Survives server restarts
#   - Docker volume mounted (zero-copy with container for /workspace/)
#   - Config: {"type": "filesystem", "root": "/path", "virtual_mode": true}
#   - Parameters:
#     * root: Directory path (defaults to project workspace if not set)
#     * virtual_mode: true = direct mount, false = copy-based (default: true)
#
# StoreBackend (/memories/hot/):
#   - In-memory data store (0.001ms access latency)
#   - Persists across agent tool calls within same session
#   - Automatically snapshotted to disk every 5 minutes (configurable)
#   - Restored from snapshot on session resume
#   - Great for current context/working memory
#   - Config: {"type": "store"}
#
# StateBackend (/tmp/):
#   - Runtime ephemeral state (0.0005ms access latency)
#   - Cleared on runtime restart
#   - Best for temporary caching
#   - Not persisted anywhere
#   - Config: {"type": "state"}
#
# Performance Characteristics:
#   FilesystemBackend write:  0.036ms (1KB)
#   StoreBackend write:       0.001ms
#   StateBackend write:       0.0005ms
#   Network file sync:        50-200ms
#   Database write:           5-50ms
#
AGENT_BACKEND_ROUTES=

# ============================================================================
# Observability (OpenTelemetry + LangSmith)
# ============================================================================
#
# Cognition supports comprehensive observability via OpenTelemetry.
# You can export traces to:
#   1. LangSmith (cloud or self-hosted) - for agent internals
#   2. Custom OTLP backends (Jaeger, Grafana Tempo, Datadog, etc.) - for infrastructure
#   3. Both simultaneously (fan-out)
#   4. Console (debug mode)
#
# All traces include correlation IDs, session/project context, and are linked to logs.
# ============================================================================

# --- Master Switch ---
# Set to true to explicitly enable tracing, or leave as false for auto-detection
# (tracing auto-enables if LANGSMITH_TRACING=true or OTEL_EXPORTER_OTLP_ENDPOINT is set)
OTEL_ENABLED=false

# --- Service Identity ---
OTEL_SERVICE_NAME=cognition

# --- Custom OTLP Backend (Jaeger, Tempo, Datadog, etc.) ---
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# OTEL_EXPORTER_OTLP_HEADERS=api-key=your-key,other-header=value

# --- LangSmith Configuration ---
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=lsv2_...
# LANGSMITH_PROJECT=cognition
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com  # Change for self-hosted
#
# Example scenarios:
#   1. Keep everything in-house:
#      OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318
#
#   2. Use LangSmith for agent tracing:
#      LANGSMITH_TRACING=true
#      LANGSMITH_API_KEY=lsv2_...
#
#   3. Fan-out to both:
#      LANGSMITH_TRACING=true
#      LANGSMITH_API_KEY=lsv2_...
#      OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318
#
#   4. Debug mode (console output):
#      OTEL_ENABLED=true
#      DEBUG=true

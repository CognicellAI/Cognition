# =============================================================================
# SERVER SETTINGS
# =============================================================================
server:
  # Host address to bind the server to
  # Default: "127.0.0.1"
  # Use "0.0.0.0" to accept connections from any IP (not recommended for security)
  # Environment variable: COGNITION_HOST
  host: "127.0.0.1"

  # Port number for the server
  # Default: 8000
  # Range: 1-65535
  # Environment variable: COGNITION_PORT
  port: 8000

  # Logging level
  # Options: "debug", "info", "warning", "error", "critical"
  # Default: "info"
  # Use "debug" for troubleshooting, "warning" for production
  # Environment variable: COGNITION_LOG_LEVEL
  log_level: "info"

  # Maximum number of concurrent sessions
  # Default: 100
  # Increase for high-traffic deployments, decrease to save memory
  # Environment variable: COGNITION_MAX_SESSIONS
#  max_sessions: 100

  # Session timeout in seconds
  # Default: 3600 (1 hour)
  # Sessions inactive for longer than this will be cleaned up
  # Environment variable: COGNITION_SESSION_TIMEOUT_SECONDS
  session_timeout_seconds: 3600

# =============================================================================
# LLM (LANGUAGE MODEL) SETTINGS
# =============================================================================
llm:
  # LLM provider to use
  # Options: "openai", "bedrock", "openai_compatible", "mock"
  # Default: "mock"
  #
  # - "openai": OpenAI API (requires OPENAI_API_KEY)
  # - "bedrock": AWS Bedrock (requires AWS credentials)
  # - "openai_compatible": OpenAI-compatible API (OpenRouter, Ollama, etc.)
  # - "mock": Dummy provider for testing (no API calls)
  #
  # Environment variable: COGNITION_LLM_PROVIDER
  provider: "openai_compatible"

  # Model name to use
  # Default: "gpt-4o"
  #
  # OpenAI options: "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"
  # Bedrock options: "anthropic.claude-3-sonnet-20240229-v1:0", etc.
  # OpenRouter options: "google/gemini-pro", "anthropic/claude-3-opus", etc.
  # Mock: any string (not used)
  #
  # Environment variable: COGNITION_LLM_MODEL
  model: "google/gemini-3-flash-preview"

  # Temperature controls randomness (0.0 = deterministic, 2.0 = very random)
  # Default: not set (uses provider default, usually 0.7)
  # Range: 0.0 to 2.0
  # Lower for coding tasks (0.0-0.3), higher for creative tasks (0.7-1.0)
#  temperature: 0.7

  # Maximum tokens per response
  # Default: not set (uses provider default)
  # Set lower to reduce costs and response time
  # Examples: 1024, 2048, 4096, 8192
#  max_tokens: 4096

  # System prompt sent to the LLM on every request
  # Default: not set (uses built-in default)
  # Customize for your specific use case
  system_prompt: |
    You are helping with a Python project.
    Follow PEP 8 style guidelines and include type hints.
    Write tests for new functions.

# OPENAI-COMPATIBLE PROVIDER SETTINGS
# =============================================================================
# These settings are used when llm.provider is "openai_compatible"
# Works with OpenRouter, Ollama, local LLM servers, etc.

openai_compatible:
  # Base URL for the API
  # Examples:
  #   - OpenRouter: "https://openrouter.ai/api/v1"
  #   - Ollama: "http://localhost:11434/v1"
  #   - Local server: "http://localhost:8000/v1"
  # Environment variable: COGNITION_OPENAI_COMPATIBLE_BASE_URL
  base_url: "https://openrouter.ai/api/v1"
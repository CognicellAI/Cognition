# Docker Compose for Cognition with Full Observability Stack
#
# Usage:
#   docker-compose up -d          # Start all services
#   docker-compose logs -f        # View logs
#   docker-compose down           # Stop all services
#   docker-compose down -v        # Stop and remove volumes
#
# Services:
#   - cognition:       FastAPI server (port 8000)
#   - postgres:        PostgreSQL database (port 5432)
#   - mlflow:          MLflow tracking server (port 5000)
#   - prometheus:      Metrics collection (port 9091)
#   - grafana:         Dashboards (port 3000)
#   - otel-collector:  OpenTelemetry trace collector
#   - loki:            Log aggregation
#   - promtail:        Log shipping

services:
  # ──────────────────────────────────────────────────────
  # Layer 2: Persistence
  # ──────────────────────────────────────────────────────

  # PostgreSQL - Primary persistence backend
  postgres:
    image: postgres:16-alpine
    container_name: cognition-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-cognition}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-cognition}
      POSTGRES_DB: ${POSTGRES_DB:-cognition}
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - cognition-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-cognition}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ──────────────────────────────────────────────────────
  # Layer 4: Agent Runtime (Cognition Server)
  # ──────────────────────────────────────────────────────

  # Cognition Server
  cognition:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cognition-server
    ports:
      - "8000:8000"
      - "9090:9090"
    environment:
      # Server
      - COGNITION_HOST=0.0.0.0
      - COGNITION_PORT=8000
      - COGNITION_LOG_LEVEL=info
      - COGNITION_LOG_FORMAT=json
      - COGNITION_WORKSPACE_ROOT=/workspace
      - COGNITION_METRICS_PORT=9090

      # LLM Provider
      - COGNITION_LLM_PROVIDER=openai_compatible
      - COGNITION_LLM_MODEL=google/gemini-3-flash-preview
      - COGNITION_OPENAI_COMPATIBLE_BASE_URL=https://openrouter.ai/api/v1
      - COGNITION_OPENAI_COMPATIBLE_API_KEY=${COGNITION_OPENAI_COMPATIBLE_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

      # Persistence — PostgreSQL
      - COGNITION_PERSISTENCE_BACKEND=postgres
      - COGNITION_PERSISTENCE_URI=postgresql+asyncpg://${POSTGRES_USER:-cognition}:${POSTGRES_PASSWORD:-cognition}@postgres:5432/${POSTGRES_DB:-cognition}

      # MLflow — Tracing & Evaluation
      - COGNITION_MLFLOW_ENABLED=true
      - COGNITION_MLFLOW_TRACKING_URI=http://mlflow:5000
      - COGNITION_MLFLOW_EXPERIMENT_NAME=cognition

      # Sandbox — Local backend (commands execute inside this container)
      # For isolated Docker sandbox execution, run Cognition on the host
      # with COGNITION_SANDBOX_BACKEND=docker instead.
      - COGNITION_SANDBOX_BACKEND=local

      # Session Scoping — Multi-tenant isolation
      - COGNITION_SCOPING_ENABLED=true
      - COGNITION_SCOPE_KEYS=["user"]

      # OpenTelemetry — OTel Collector
      - COGNITION_OTEL_ENABLED=true
      - COGNITION_OTEL_ENDPOINT=http://otel-collector:4317
      - LANGSMITH_TRACING=false
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_EXPORTER_OTLP_PROTOCOL=grpc
      - OTEL_SERVICE_NAME=cognition
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_LOG_LEVEL=debug
    volumes:
      - ./workspaces:/workspace
      - ./.cognition:/app/.cognition
      - ./.cognition:/workspace/.cognition
      - ./tests:/app/tests
      - mlflow-artifacts:/mlflow/artifacts
    networks:
      - cognition-network
    depends_on:
      postgres:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # ──────────────────────────────────────────────────────
  # Layer 7: Observability
  # ──────────────────────────────────────────────────────

  # MLflow - Experiment Tracking & Evaluation
  mlflow:
    build:
      context: .
      dockerfile_inline: |
        FROM ghcr.io/mlflow/mlflow:v3.10.0
        RUN pip install --no-cache-dir psycopg2-binary
    container_name: cognition-mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql://${POSTGRES_USER:-cognition}:${POSTGRES_PASSWORD:-cognition}@postgres:5432/${POSTGRES_DB:-cognition}
      --default-artifact-root /mlflow/artifacts
      --disable-security-middleware
    ports:
      - "5050:5000"
    volumes:
      - mlflow-artifacts:/mlflow/artifacts
    networks:
      - cognition-network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: unless-stopped

  # Prometheus - Metrics Collection
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: cognition-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "9091:9090"
    volumes:
      - ./docker/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - cognition-network
    depends_on:
      - cognition
    restart: unless-stopped

  # Grafana - Dashboards and Visualization
  grafana:
    image: grafana/grafana:10.2.3
    container_name: cognition-grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=grafana-lokiexplore-app
    ports:
      - "3000:3000"
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    networks:
      - cognition-network
    depends_on:
      - prometheus
      - loki
    restart: unless-stopped

  # OpenTelemetry Collector - Central trace collector
  # Receives OTel traces and fans out to MLflow (for eval/prompt registry)
  # and Grafana (for centralized observability)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.96.0
    container_name: cognition-otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    ports:
      - "4317:4317"     # OTLP gRPC
      - "4318:4318"     # OTLP HTTP
    volumes:
      - ./docker/otel-collector-config.yml:/etc/otel-collector-config.yml:ro
    networks:
      - cognition-network
    depends_on:
      mlflow:
        condition: service_healthy
    restart: unless-stopped

  # Loki - Log Aggregation
  loki:
    image: grafana/loki:2.9.3
    container_name: cognition-loki
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    volumes:
      - ./docker/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    networks:
      - cognition-network
    restart: unless-stopped

  # Promtail - Log Shipping
  promtail:
    image: grafana/promtail:2.9.3
    container_name: cognition-promtail
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./docker/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - cognition-network
    depends_on:
      - loki
    restart: unless-stopped

networks:
  cognition-network:
    driver: bridge

volumes:
  postgres-data:
  mlflow-artifacts:
  prometheus-data:
  grafana-data:
  loki-data:

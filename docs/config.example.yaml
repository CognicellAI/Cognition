# Cognition Configuration File
# 
# This is a complete example configuration file showing all available options.
# Copy this to ~/.cognition/config.yaml for global settings or
# .cognition/config.yaml in your project directory for project-specific settings.
#
# Configuration hierarchy (later sources override earlier ones):
#   1. Built-in defaults
#   2. ~/.cognition/config.yaml (this file if placed there)
#   3. .cognition/config.yaml (in project directory)
#   4. Environment variables (highest precedence)

# =============================================================================
# SERVER SETTINGS
# =============================================================================
server:
  # Host address to bind the server to
  # Default: "127.0.0.1"
  # Use "0.0.0.0" to accept connections from any IP (not recommended for security)
  # Environment variable: COGNITION_HOST
  host: "127.0.0.1"

  # Port number for the server
  # Default: 8000
  # Range: 1-65535
  # Environment variable: COGNITION_PORT
  port: 8000

  # Logging level
  # Options: "debug", "info", "warning", "error", "critical"
  # Default: "info"
  # Use "debug" for troubleshooting, "warning" for production
  # Environment variable: COGNITION_LOG_LEVEL
  log_level: "info"

  # Maximum number of concurrent sessions
  # Default: 100
  # Increase for high-traffic deployments, decrease to save memory
  # Environment variable: COGNITION_MAX_SESSIONS
  max_sessions: 100

  # Session timeout in seconds
  # Default: 3600 (1 hour)
  # Sessions inactive for longer than this will be cleaned up
  # Environment variable: COGNITION_SESSION_TIMEOUT_SECONDS
  session_timeout_seconds: 3600

# =============================================================================
# LLM (LANGUAGE MODEL) SETTINGS
# =============================================================================
llm:
  # LLM provider to use
  # Options: "openai", "bedrock", "openai_compatible", "mock"
  # Default: "mock"
  #
  # - "openai": OpenAI API (requires OPENAI_API_KEY)
  # - "bedrock": AWS Bedrock (requires AWS credentials)
  # - "openai_compatible": OpenAI-compatible API (OpenRouter, Ollama, etc.)
  # - "mock": Dummy provider for testing (no API calls)
  #
  # Environment variable: COGNITION_LLM_PROVIDER
  provider: "mock"

  # Model name to use
  # Default: "gpt-4o"
  #
  # OpenAI options: "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"
  # Bedrock options: "anthropic.claude-3-sonnet-20240229-v1:0", etc.
  # OpenRouter options: "google/gemini-pro", "anthropic/claude-3-opus", etc.
  # Mock: any string (not used)
  #
  # Environment variable: COGNITION_LLM_MODEL
  model: "gpt-4o"

  # Temperature controls randomness (0.0 = deterministic, 2.0 = very random)
  # Default: not set (uses provider default, usually 0.7)
  # Range: 0.0 to 2.0
  # Lower for coding tasks (0.0-0.3), higher for creative tasks (0.7-1.0)
  temperature: 0.7

  # Maximum tokens per response
  # Default: not set (uses provider default)
  # Set lower to reduce costs and response time
  # Examples: 1024, 2048, 4096, 8192
  max_tokens: 4096

  # System prompt sent to the LLM on every request
  # Default: not set (uses built-in default)
  # Customize for your specific use case
  system_prompt: |
    You are a helpful AI coding assistant. You help users write, debug, and improve code.
    Always write clean, well-documented code following best practices.

# =============================================================================
# OPENAI PROVIDER SETTINGS
# =============================================================================
# These settings are only used when llm.provider is "openai"
#
# NOTE: Sensitive values (API keys) should be set via environment variables
# or .env file, not in this config file, to avoid exposing secrets.

openai:
  # OpenAI API base URL
  # Default: "https://api.openai.com/v1"
  # Change this for OpenAI-compatible endpoints or proxies
  # Environment variable: OPENAI_API_BASE
  api_base: "https://api.openai.com/v1"

# =============================================================================
# AWS BEDROCK PROVIDER SETTINGS
# =============================================================================
# These settings are only used when llm.provider is "bedrock"
#
# NOTE: Sensitive values should be set via environment variables

bedrock:
  # AWS region for Bedrock
  # Default: "us-east-1"
  # Other options: "us-west-2", "eu-west-1", etc.
  # Environment variable: AWS_REGION
  region: "us-east-1"

  # Bedrock model ID
  # Default: "anthropic.claude-3-sonnet-20240229-v1:0"
  # Other options:
  #   - "anthropic.claude-3-opus-20240229-v1:0"
  #   - "anthropic.claude-3-haiku-20240307-v1:0"
  #   - "amazon.titan-text-express-v1"
  # Environment variable: COGNITION_BEDROCK_MODEL_ID
  model_id: "anthropic.claude-3-sonnet-20240229-v1:0"

# =============================================================================
# OPENAI-COMPATIBLE PROVIDER SETTINGS
# =============================================================================
# These settings are used when llm.provider is "openai_compatible"
# Works with OpenRouter, Ollama, local LLM servers, etc.

openai_compatible:
  # Base URL for the API
  # Examples:
  #   - OpenRouter: "https://openrouter.ai/api/v1"
  #   - Ollama: "http://localhost:11434/v1"
  #   - Local server: "http://localhost:8000/v1"
  # Environment variable: COGNITION_OPENAI_COMPATIBLE_BASE_URL
  base_url: "https://openrouter.ai/api/v1"

# =============================================================================
# OLLAMA SETTINGS
# =============================================================================
# For local LLM testing with Ollama (use with openai_compatible provider)

ollama:
  # Ollama model name
  # Default: "llama3.2"
  # Other options: "mistral", "codellama", "phi", etc.
  # Environment variable: COGNITION_OLLAMA_MODEL
  model: "llama3.2"

  # Ollama server URL
  # Default: "http://localhost:11434"
  # Change if Ollama runs on different host/port
  # Environment variable: COGNITION_OLLAMA_BASE_URL
  base_url: "http://localhost:11434"

# =============================================================================
# WORKSPACE SETTINGS
# =============================================================================

workspace:
  # Root directory for project workspaces
  # Default: "./workspaces"
  # All project files are stored under this directory
  # Make sure this directory exists and is writable
  # Environment variable: COGNITION_WORKSPACE_ROOT
  root: "./workspaces"

# =============================================================================
# RATE LIMITING SETTINGS
# =============================================================================
# Protects the server from abuse by limiting requests per client

rate_limit:
  # Maximum requests per minute per client
  # Default: 60
  # Increase for high-traffic scenarios, decrease for stricter limits
  # Environment variable: COGNITION_RATE_LIMIT_PER_MINUTE
  per_minute: 60

  # Burst allowance (instant requests allowed)
  # Default: 10
  # Allows short bursts above the per-minute rate
  # Environment variable: COGNITION_RATE_LIMIT_BURST
  burst: 10

# =============================================================================
# OBSERVABILITY SETTINGS
# =============================================================================
# For monitoring, metrics, and distributed tracing

observability:
  # OpenTelemetry collector endpoint
  # Default: null (disabled)
  # Example: "http://localhost:4317" or "http://otel-collector:4317"
  # Set to enable distributed tracing
  # Environment variable: COGNITION_OTEL_ENDPOINT
  otel_endpoint: null

  # Prometheus metrics port
  # Default: 9090
  # Metrics available at http://localhost:9090/metrics
  # Environment variable: COGNITION_METRICS_PORT
  metrics_port: 9090

# =============================================================================
# TESTING SETTINGS
# =============================================================================
# These are typically only used in test environments

test:
  # LLM mode for testing
  # Options: "mock", "openai", "ollama"
  # Default: "mock"
  # Use "mock" for fast tests without API calls
  # Use "openai" or "ollama" for integration tests
  # Environment variable: COGNITION_TEST_LLM_MODE
  llm_mode: "mock"

# =============================================================================
# AGENT SETTINGS
# =============================================================================
# Configure the AI agent behavior

agent:
  # Maximum number of iterations for multi-step tasks
  # Default: 15
  # Increase for complex tasks, decrease to limit token usage
  max_iterations: 15

  # Enable/disable specific tool categories
  # All enabled by default
  tools:
    filesystem: true    # File read/write operations
    git: true           # Git operations (status, diff, log)
    search: true        # Code search (grep, find)
    test: true          # Test runners (pytest, jest)
    lint: true          # Linters (ruff, eslint)

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
# Note: Most security settings are configured via environment variables
# to avoid storing secrets in config files

security:
  # Allowed CORS origins (for browser clients)
  # Default: [] (CORS disabled)
  # Example: ["http://localhost:3000", "https://myapp.com"]
  cors_origins: []

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================
# Uncomment the appropriate section for your use case:

# --- Example 1: OpenAI (Recommended for most users) ---
# llm:
#   provider: openai
#   model: gpt-4o
#   temperature: 0.7
#   system_prompt: "You are a helpful coding assistant."
# # Set OPENAI_API_KEY in your .env file

# --- Example 2: AWS Bedrock ---
# llm:
#   provider: bedrock
#   model: anthropic.claude-3-sonnet-20240229-v1:0
#   temperature: 0.5
# # Set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION in .env

# --- Example 3: OpenRouter (multiple models) ---
# llm:
#   provider: openai_compatible
#   model: google/gemini-pro
# openai_compatible:
#   base_url: "https://openrouter.ai/api/v1"
# # Set COGNITION_OPENAI_COMPATIBLE_API_KEY in .env

# --- Example 4: Ollama (local, free) ---
# llm:
#   provider: openai_compatible
#   model: llama3.2
# openai_compatible:
#   base_url: "http://localhost:11434/v1"

# --- Example 5: Mock (testing, no API calls) ---
# llm:
#   provider: mock
#   model: mock-model

# =============================================================================
# ENVIRONMENT VARIABLES REFERENCE
# =============================================================================
# 
# Required for specific providers:
#   OPENAI_API_KEY                  # Required for OpenAI provider
#   AWS_ACCESS_KEY_ID               # Required for Bedrock provider
#   AWS_SECRET_ACCESS_KEY           # Required for Bedrock provider
#   COGNITION_OPENAI_COMPATIBLE_API_KEY  # Required for compatible provider
#
# Optional overrides (these override config file values):
#   COGNITION_HOST                  # Server host
#   COGNITION_PORT                  # Server port
#   COGNITION_LOG_LEVEL             # Logging level
#   COGNITION_WORKSPACE_ROOT        # Workspace directory
#   COGNITION_LLM_PROVIDER          # LLM provider
#   COGNITION_LLM_MODEL             # Model name
#   COGNITION_MAX_SESSIONS          # Max concurrent sessions
#   COGNITION_SESSION_TIMEOUT_SECONDS  # Session timeout
#   COGNITION_RATE_LIMIT_PER_MINUTE  # Rate limit
#   COGNITION_RATE_LIMIT_BURST       # Rate limit burst
#   COGNITION_OTEL_ENDPOINT          # OpenTelemetry endpoint
#   COGNITION_METRICS_PORT           # Metrics port
#   COGNITION_TEST_LLM_MODE          # Test LLM mode
#
# Create a .env file in your project root with these variables:
#
#   OPENAI_API_KEY=sk-...
#   COGNITION_LLM_PROVIDER=openai
#   COGNITION_LOG_LEVEL=debug
